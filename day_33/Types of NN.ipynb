{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Types of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working on the neural network it is essential to know the application of Neural Network and where to use what type of NN.\n",
    "\n",
    "following are the example of neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised learning NN**\n",
    "\n",
    "<center>______________________________Standard Neural network ______________________________\n",
    "   \n",
    "1. Structured data\n",
    "2. Market pricing (sales)\n",
    "3. Online advertisement etc.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<center>_________________________ _____Convolutional (CNN) ________________________________________\n",
    "\n",
    "1. Image classification\n",
    "2. video classification etc.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<center>____________________________Recurrent (RNN)__________________________________________\n",
    "\n",
    "1. Speech recognition\n",
    "2. Text \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<center>_______________________________ Customized or  Hybrid______________________________-\n",
    "\n",
    "1. used in autonomous driving (car)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Convolutional NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Convolutional Neural Network (CNN)** is a specialized type of artificial neural network designed primarily for processing structured grid data, such as images. CNNs are especially effective in tasks like image recognition, object detection, and natural language processing.\n",
    "\n",
    "### Key Components of CNNs\n",
    "\n",
    "1. **Convolutional Layers**:\n",
    "   - These layers perform convolutions, a mathematical operation that applies a filter (or kernel) to input data.\n",
    "   - Filters slide over the input data, extracting features such as edges, textures, and shapes.\n",
    "   - The output is a feature map that highlights the presence of these features.\n",
    "\n",
    "2. **Pooling Layers**:\n",
    "   - Pooling layers down-sample feature maps to reduce dimensions while retaining important features.\n",
    "   - Common types:\n",
    "     - **Max Pooling**: Retains the maximum value in each pool region.\n",
    "     - **Average Pooling**: Takes the average of each pool region.\n",
    "\n",
    "3. **Activation Functions**:\n",
    "   - Non-linear functions like ReLU (Rectified Linear Unit) introduce non-linearity, enabling the network to learn complex patterns.\n",
    "   - ReLU replaces negative values with zero while keeping positive values unchanged.\n",
    "\n",
    "4. **Fully Connected Layers**:\n",
    "   - These layers connect every neuron from the previous layer to every neuron in the next.\n",
    "   - They integrate features extracted by convolutional and pooling layers to make predictions.\n",
    "\n",
    "5. **Dropout (Optional)**:\n",
    "   - Dropout is a regularization technique to prevent overfitting by randomly setting a fraction of the layer's nodes to zero during training.\n",
    "\n",
    "6. **Output Layer**:\n",
    "   - Produces the final predictions using activation functions like **softmax** (for multi-class classification) or **sigmoid** (for binary classification).\n",
    "\n",
    "---\n",
    "\n",
    "### How CNNs Work\n",
    "\n",
    "1. **Input Layer**: The image is input as a 2D or 3D array (width × height × channels, e.g., RGB has three channels).\n",
    "\n",
    "2. **Feature Extraction**:\n",
    "   - Convolutional and pooling layers extract increasingly abstract features at each layer.\n",
    "   - Early layers might detect simple patterns like edges, while deeper layers capture complex structures like objects or faces.\n",
    "\n",
    "3. **Classification**:\n",
    "   - The fully connected layers take the feature map as input and classify it into the desired categories.\n",
    "\n",
    "---\n",
    "\n",
    "### Applications of CNNs\n",
    "1. **Image Processing**: Face recognition, object detection, segmentation.\n",
    "2. **Healthcare**: Analyzing medical images like X-rays and MRIs.\n",
    "3. **Autonomous Vehicles**: Road and object detection.\n",
    "4. **Natural Language Processing**: Text and document classification.\n",
    "5. **Video Analysis**: Activity recognition and frame segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "### Popular Architectures\n",
    "1. **LeNet-5**: Early CNN for handwritten digit recognition.\n",
    "2. **AlexNet**: Introduced deeper networks and ReLU.\n",
    "3. **VGGNet**: Used very deep networks with small filters.\n",
    "4. **ResNet**: Introduced skip connections to enable deeper networks.\n",
    "5. **YOLO**: Real-time object detection.\n",
    "6. **U-Net**: Used in image segmentation tasks.\n",
    "\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://learnopencv.com/wp-content/uploads/2023/01/Convolutional-Neural-Networks.png\"><img src=\"https://learnopencv.com/wp-content/uploads/2023/01/Convolutional-Neural-Networks.png\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Standard NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Standard Neural Network**, also known as a **Fully Connected Neural Network (FCNN)** or a **Dense Neural Network (DNN)**, is a basic type of artificial neural network. It forms the foundation for understanding more advanced architectures like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Components of a Standard Neural Network\n",
    "\n",
    "1. **Input Layer**:\n",
    "   - This layer takes in the raw input data (e.g., numerical features, pixels of an image, etc.).\n",
    "   - The number of neurons in the input layer equals the number of features in the input data.\n",
    "\n",
    "2. **Hidden Layers**:\n",
    "   - Layers between the input and output layers where computations occur.\n",
    "   - Each neuron in a hidden layer performs a weighted sum of its inputs, adds a bias, and passes the result through an **activation function**.\n",
    "   - More hidden layers or neurons can allow the network to learn complex patterns (deep networks).\n",
    "\n",
    "3. **Output Layer**:\n",
    "   - Produces the final output of the network.\n",
    "   - The number of neurons depends on the task:\n",
    "     - Single neuron for regression or binary classification.\n",
    "     - Multiple neurons (equal to the number of classes) for multi-class classification.\n",
    "\n",
    "4. **Weights and Biases**:\n",
    "   - Weights are the parameters that connect neurons between layers. They determine the importance of each input.\n",
    "   - Biases allow the model to shift the activation function, increasing flexibility in learning patterns.\n",
    "\n",
    "5. **Activation Functions**:\n",
    "   - Introduce non-linearity, enabling the network to learn complex mappings.\n",
    "   - Common activation functions include:\n",
    "     - **Sigmoid**: Outputs values between 0 and 1.\n",
    "     - **ReLU (Rectified Linear Unit)**: Replaces negative values with zero, speeding up training.\n",
    "     - **Tanh**: Outputs values between -1 and 1.\n",
    "     - **Softmax**: Used in the output layer for multi-class classification.\n",
    "\n",
    "6. **Loss Function**:\n",
    "   - Measures how well the network’s predictions match the actual outputs.\n",
    "   - Common loss functions:\n",
    "     - Mean Squared Error (MSE) for regression tasks.\n",
    "     - Cross-Entropy Loss for classification tasks.\n",
    "\n",
    "7. **Optimizer**:\n",
    "   - Updates the weights and biases during training to minimize the loss.\n",
    "   - Common optimizers include:\n",
    "     - **Stochastic Gradient Descent (SGD)**\n",
    "     - **Adam**\n",
    "\n",
    "---\n",
    "\n",
    "### How a Standard Neural Network Works\n",
    "\n",
    "1. **Forward Propagation**:\n",
    "   - Input data is passed through the network layer by layer.\n",
    "   - At each neuron:\n",
    "     - \\( z = \\sum (w \\cdot x) + b \\) (weighted sum + bias)\n",
    "     - \\( a = \\text{ActivationFunction}(z) \\)\n",
    "\n",
    "2. **Loss Calculation**:\n",
    "   - The predicted output is compared with the true labels using the loss function.\n",
    "\n",
    "3. **Backpropagation**:\n",
    "   - The network computes gradients of the loss with respect to each weight and bias using the chain rule.\n",
    "   - These gradients are used to update the weights in the direction that minimizes the loss.\n",
    "\n",
    "4. **Weight Update**:\n",
    "   - Optimizers adjust the weights and biases using the gradients:\n",
    "     - \\( w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\nabla \\text{Loss} \\)\n",
    "       - \\( \\eta \\): Learning rate.\n",
    "\n",
    "5. **Iteration**:\n",
    "   - The process of forward propagation, loss calculation, backpropagation, and weight update repeats over multiple epochs until the model converges.\n",
    "\n",
    "---\n",
    "\n",
    "### Applications of Standard Neural Networks\n",
    "\n",
    "1. **Regression Problems**:\n",
    "   - Predicting continuous values like house prices or stock prices.\n",
    "   \n",
    "2. **Classification Problems**:\n",
    "   - Binary or multi-class classification like spam detection or sentiment analysis.\n",
    "\n",
    "3. **Simple Pattern Recognition**:\n",
    "   - Recognizing relationships in structured data (e.g., tabular data).\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations of Standard Neural Networks\n",
    "1. **Scalability**: They are not ideal for high-dimensional data like images or videos because they treat all input features equally, leading to a massive number of parameters.\n",
    "2. **Overfitting**: They are prone to overfitting, especially when there are many parameters relative to the data.\n",
    "3. **Data Structure**: They do not consider spatial or sequential relationships in data (unlike CNNs or RNNs).\n",
    "\n",
    "---\n",
    "\n",
    "<a target=\"_blank\" href=\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/20230602113310/Neural-Networks-Architecture.png\"><img src=\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/20230602113310/Neural-Networks-Architecture.png\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Recurrent (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Recurrent Neural Network (RNN)** is a type of artificial neural network designed for sequential data, such as time series, natural language, or audio signals. RNNs are particularly effective for tasks where the order of data points matters, as they incorporate information from previous steps (context) when processing current inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Features of RNNs\n",
    "\n",
    "1. **Recurrent Connections**:\n",
    "   - Unlike feedforward networks, RNNs have connections that loop back, allowing them to pass information from one time step to the next.\n",
    "   - This structure enables them to maintain a form of memory of past inputs.\n",
    "\n",
    "2. **Sequential Processing**:\n",
    "   - RNNs process input sequences step by step.\n",
    "   - At each step, the network takes in the current input and the hidden state from the previous step.\n",
    "\n",
    "3. **Shared Weights**:\n",
    "   - The same weights are used across all time steps, reducing the number of parameters and allowing the network to generalize across different sequence lengths.\n",
    "\n",
    "4. **Hidden State**:\n",
    "   - The hidden state serves as the memory of the network, storing information from previous time steps.\n",
    "\n",
    "---\n",
    "\n",
    "### How an RNN Works\n",
    "\n",
    "1. **At Each Time Step \\( t \\)**:\n",
    "   - Input: \\( x_t \\)\n",
    "   - Hidden State: \\( h_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h) \\)\n",
    "     - \\( W_{xh} \\): Weight matrix for input to hidden state.\n",
    "     - \\( W_{hh} \\): Weight matrix for hidden to hidden state.\n",
    "     - \\( b_h \\): Bias term.\n",
    "     - \\( f \\): Activation function, often \\( \\tanh \\) or ReLU.\n",
    "   - Output: \\( y_t = g(W_{hy}h_t + b_y) \\)\n",
    "     - \\( W_{hy} \\): Weight matrix for hidden state to output.\n",
    "     - \\( g \\): Activation function (e.g., softmax for classification).\n",
    "\n",
    "2. **Backpropagation Through Time (BPTT)**:\n",
    "   - The RNN is trained using a variation of backpropagation that unfolds the network across all time steps.\n",
    "   - Gradients are calculated for all time steps and used to update weights.\n",
    "   - A challenge is the **vanishing gradient problem**, where gradients diminish, making it hard to learn long-term dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "### Variants of RNNs\n",
    "1. **Long Short-Term Memory (LSTM)**:\n",
    "   - Addresses the vanishing gradient problem by introducing gates to control the flow of information:\n",
    "     - **Forget Gate**: Decides what to discard from memory.\n",
    "     - **Input Gate**: Decides what new information to store.\n",
    "     - **Output Gate**: Decides what information to output.\n",
    "   - LSTMs excel at learning long-term dependencies.\n",
    "\n",
    "2. **Gated Recurrent Unit (GRU)**:\n",
    "   - A simplified version of LSTM with fewer parameters.\n",
    "   - Combines the forget and input gates into a single update gate.\n",
    "\n",
    "3. **Bidirectional RNN (BiRNN)**:\n",
    "   - Processes data in both forward and backward directions.\n",
    "   - Useful for tasks where both past and future context are important (e.g., language translation).\n",
    "\n",
    "4. **Attention Mechanisms**:\n",
    "   - RNNs can be enhanced with attention to focus on relevant parts of the input sequence when making predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### Applications of RNNs\n",
    "\n",
    "1. **Natural Language Processing**:\n",
    "   - Text generation\n",
    "   - Language modeling\n",
    "   - Machine translation\n",
    "   - Sentiment analysis\n",
    "\n",
    "2. **Time Series Analysis**:\n",
    "   - Stock price prediction\n",
    "   - Weather forecasting\n",
    "\n",
    "3. **Speech and Audio Processing**:\n",
    "   - Speech recognition\n",
    "   - Music generation\n",
    "\n",
    "4. **Video Analysis**:\n",
    "   - Activity recognition\n",
    "   - Video captioning\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations of RNNs\n",
    "\n",
    "1. **Vanishing Gradient Problem**:\n",
    "   - Difficult to learn long-term dependencies in standard RNNs.\n",
    "\n",
    "2. **Training Time**:\n",
    "   - RNNs can be slow to train due to sequential processing.\n",
    "\n",
    "3. **Overfitting**:\n",
    "   - Prone to overfitting when the training dataset is small.\n",
    "\n",
    "---\n",
    "\n",
    "<a target=\"_blank\" href=\"https://media.geeksforgeeks.org/wp-content/uploads/20231204125839/What-is-Recurrent-Neural-Network-660.webp\"><img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231204125839/What-is-Recurrent-Neural-Network-660.webp\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
